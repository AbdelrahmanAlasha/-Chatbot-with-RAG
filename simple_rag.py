# -*- coding: utf-8 -*-
"""Simple RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17648WQwViFyrv0iGarob6znNIZFfWJHO
"""

!pip install faiss-cpu sentence-transformers transformers accelerate pdfplumber

from google.colab import files

uploaded = files.upload()  # Upload PDFs or TXT files

import pdfplumber
def extract_text_from_pdf(path):
    text = ""
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                text += t + "\n"
    return text

docs = []
for fname in uploaded:
    if fname.endswith(".pdf"):
        text = extract_text_from_pdf(fname)
    else:
        with open(fname, "r", encoding="utf-8") as f:
            text = f.read()
    docs.append(text)

full_text = "\n".join(docs)
print("‚úÖ Loaded all text.")

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

import nltk
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize

def smart_chunk_text(text, chunk_size=300):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_len = 0

    for sentence in sentences:
        tokens = sentence.split()
        if current_len + len(tokens) > chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = tokens
            current_len = len(tokens)
        else:
            current_chunk.extend(tokens)
            current_len += len(tokens)

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

chunks = smart_chunk_text(full_text)


print(f"üì¶ Created {len(chunks)} chunks.")

embedder = SentenceTransformer("multi-qa-MiniLM-L6-cos-v1")
embeddings = embedder.encode(chunks, show_progress_bar=True)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(np.array(embeddings))

from transformers import pipeline
from sklearn.preprocessing import normalize

llm = pipeline("text-generation", model="tiiuae/falcon-7b-instruct", max_new_tokens=512)

def ask_question(question, top_k=3):
    q_emb = normalize(embedder.encode([question]))
    D, I = index.search(np.array(q_emb), top_k)
    context = "\n\n".join([f"Chunk {i+1}:\n{chunks[i]}" for i in I[0]])

    prompt = f"""Answer the question based only on the context below. If the answer is not contained, say you don't know.

              Context:
              {context}

              Question: {question}

              Answer:"""

    response = llm(prompt)[0]["generated_text"]
    return response[len(prompt):].strip()  # Strip the prompt from the output



while True:
    q = input("‚ùì Question: ")
    if q.lower() in ["exit", "quit"]:
        break
    answer = ask_question(q)
    print("\nüí° Answer:\n")
    print(answer)

